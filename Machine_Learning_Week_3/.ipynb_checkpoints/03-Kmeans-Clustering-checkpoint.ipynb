{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Unsupervised learning - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is when we **do not** provide the machine with labeled data, and the machine **derives structures** from the data all on it's own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://i.imgur.com/ExLe6KS.jpg\" alt=\"unsupervised\" style=\"width: 800px;\"/>\n",
    "<figcaption>Source: boozallen.com</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the process of grouping data into _clusters_ based on certain similarities in data. It provides insight into the inherant groups in data such as grouping customers based on their purchase data.\n",
    "\n",
    "Example:\n",
    "1. Social Network: Clustering user into different communitied based on their 'likes' and interests\n",
    "2. Marketing: Cluster customers into groups of similar buying groups (targeted ads)\n",
    "3. Medicine: Cluster patients into groups with similar symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform clustering, we must decide on few things:\n",
    "1. Clustering criterion\n",
    "2. Proximity measure     \n",
    "3. Clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clustering criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the problem - _What type of cluster do you expect to find in your data?_ Same data can be clustered in different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr></tr>\n",
    "    <tr>      \n",
    "        <td><figure><img src=\"https://i.imgur.com/jUBh4nb.png\" alt=\"clustering criterion\" style=\"width: 300px;\"/><figcaption>(a) clustered based on the environment in which the animals live</figcaption></figure></td>\n",
    "        <td><figure><img src=\"https://i.imgur.com/OPieyZX.png\" alt=\"clustering criterion\" style=\"width: 300px;\"/><figcaption>(b) clustered based on the existence of lungs</figcaption></figure></td>                \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert knowledge might be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Proximity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group data points together, we need a notion of 'similarity'. Mathematically speaking, we need to choose an appropriate distance measure. One of the most commonly used distance measures is _Euclidean Distance_.\n",
    "\n",
    "**Euclidean distance**: Euclidean distance between two points (_x<sub>1</sub>_,_y<sub>1</sub>_) and (_x<sub>2</sub>_,_y<sub>2</sub>_) (in case of 2-dimensional space) is given as:\n",
    "<img src=\"https://i.imgur.com/Mzx2iya.png\" alt=\"euclidean\" style=\"width: 200px;\"/>\n",
    "\n",
    "<img src=\"https://i.imgur.com/fZTRe3B.png\" alt=\"euclidean\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main types of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://i.imgur.com/m37rU3o.gif\" alt=\"clustering\" style=\"width: 400px;\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means clustering (https://en.wikipedia.org/wiki/K-means_clustering) is one of the most popular clustering algorithms. It is a sequential clustering algorithm, which clusters the data into K clusters. The value of K is user-defined.\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "K-means works iteratively to assign each data point to one of the K clusters by computing feature similarity. Clustering is done in 4 steps:\n",
    "1. **Initialize**: Randomly intialize K centroids (each centroid represents a cluster). C = c<sub>1</sub>, c<sub>2</sub>,.., c<sub>k</sub>\n",
    "2. **Assign**: Assign each data point to its nearest centroid by calculating its (Euclidean) distance to each centroid. In other words, we minimize the sum of square distances between the cluster centroid and the data points \n",
    "   <img src=\"https://i.imgur.com/Mc6mSGE.jpg\" alt=\"euclidean\" style=\"width: 400px;\"/> where, <br/>\n",
    "   k = number of clusters <br/>\n",
    "   n = number of data points <br/>\n",
    "   c<sub>j</sub> = centroid of cluster j <br/>\n",
    "   (x<sub>ij</sub> - c<sub>j</sub>) = Euclidean distance between data point and centroid to which it is assigned\n",
    "   \n",
    "   \n",
    "3. **Update**: Re-compute the centroid of the clusters. This is done by taking the arithmetic mean of all the points assigned to a cluster \n",
    "4. **Iterate**: Repeat 2 and 3 until none of the cluster assignments change\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    </tr>\n",
    "    <tr>      \n",
    "        <td><figure><img src=\"https://i.imgur.com/gF2vEHJ.jpg\" alt=\"clustering criterion\" style=\"width: 300px;\"/><figcaption>1. Initialize</figcaption></figure></td>\n",
    "        <td><figure><img src=\"https://i.imgur.com/qcWJst3.jpg\" alt=\"clustering criterion\" style=\"width: 300px;\"/><figcaption>2. Assign</figcaption></figure></td>                \n",
    "    </tr>\n",
    "    <tr>\n",
    "    </tr>\n",
    "    <tr>      \n",
    "        <td><figure><img src=\"https://i.imgur.com/fSBBmf2.jpg\" alt=\"clustering criterion\" style=\"width: 300px;\"/><figcaption>3. Update</figcaption></figure></td>\n",
    "        <td><figure><img src=\"https://i.imgur.com/AJDJZUg.jpg\" alt=\"clustering criterion\" style=\"width: 300px;\"/><figcaption>4. Iterate</figcaption></figure></td>                \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This approach falls under the broad category of **Expectation Maximization** algorithms, because you try to find the _best-fit_ parameters by doing a random-initialization at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Perform clustering in iris dataset based on sepal and petal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the labels\n",
    "x = pd.DataFrame(iris.data, columns=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width'])\n",
    "y = pd.DataFrame(iris.target, columns=['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forget the labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data and see if we can visually identify any clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize the data and check if we see some patterns\n",
    "plt.rcParams['figure.figsize'] = (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw a Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x['Sepal Length'], x['Sepal Width'], s=40)\n",
    "plt.title('Sepal Length vs Sepal Width')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x['Petal Length'], x['Petal Width'], s=40)\n",
    "plt.title('Petal Length vs Petal Width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are two clusters. Let's use K-Means clustering to group these..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering using sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=2)\n",
    "x['Predicted'] = km.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ref: [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the clusters\n",
    "colors = np.array(['red', 'green'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x['Sepal Length'], x['Sepal Width'], c=colors[x['Predicted']], s=40)\n",
    "plt.title('Sepal Length vs Sepal Width')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x['Petal Length'], x['Petal Width'], c=colors[x['Predicted']], s=40)\n",
    "plt.title('Petal Length vs Petal Width')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the value of K:\n",
    "\n",
    "The main input for K-means clustering is K (the number of clusters). Optimum number of clusters might be intuitive in two dimensions (where you can visualize the data), but on higher dimensions?\n",
    "\n",
    "One of the most common methods of identifying the optimum number of K is using the **_elbow method_** where, \n",
    " - We run the algorithm for different values of k (say K = 2 to 10)\n",
    " - For each value of K, we compute the **Within Cluster Sum of Squares (WCSS)**. WCSS is defined at the sum of squared distance between each member of the cluster and its centroid <img src=\"https://i.imgur.com/FmvVsG1.jpg\" alt=\"euclidean\" style=\"width: 200px;\"/>\n",
    " For example, WCSS for the below cluster would be\n",
    " <img src=\"https://i.imgur.com/oZnw5M5.jpg\" alt=\"euclidean\" style=\"width: 600px;\"/>\n",
    " <img src=\"https://i.imgur.com/3NUP2Qy.jpg\" alt=\"euclidean\" style=\"width: 1000px;\"/>\n",
    " - Plot the K-values against the corresponding WCSS values\n",
    " - Select the value of K at which WCSS starts to level-off (i.e., at the elbow point)\n",
    " <img src=\"https://i.imgur.com/1x2EJOI.jpg\" alt=\"euclidean\" style=\"width: 600px;\"/>\n",
    "\n",
    "Note: WCSS is just one way for evaluating k-means clustering. Check out the other methods [here](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check this on Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i)\n",
    "    kmeans.fit(x)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The optimum cluster is where the elbow occurs. This is where the within-cluster sum of squares (WCSS) doesn't change significantly with every iteration. Now that we have the optimum number of clusters, let's apply this to the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3)\n",
    "x['Predicted'] = km.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw a Scatter plot for Sepal Length vs Sepal Width\n",
    "colors = np.array(['red', 'green','blue'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x['Sepal Length'], x['Sepal Width'], c=colors[x['Predicted']], s=40)\n",
    "plt.title('Sepal Length vs Sepal Width - Predicted')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x['Petal Length'], x['Petal Width'], c=colors[x['Predicted']], s=40)\n",
    "plt.title('Petal Length vs Petal Width - Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's compare with the actual labels\n",
    "\n",
    "**Disclaimer**: This is just for testing purpose, this step is **not** possible in real scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw a Scatter plot for Sepal Length vs Sepal Width\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x['Sepal Length'], x['Sepal Width'], c=colors[y['Target']], s=40)\n",
    "plt.title('Sepal Length vs Sepal Width - Actual')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x['Petal Length'], x['Petal Width'], c=colors[y['Target']], s=40)\n",
    "plt.title('Petal Length vs Petal Width - Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Points to remember..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random initialization trap: Each run of K-means might produce different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try this out with a dataset of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Impact of scaling\n",
    "\n",
    "Re-scaling your data might give entirely different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rnorm = np.random.randn\n",
    "\n",
    "x = rnorm(1000) * 10  \n",
    "y = np.concatenate([rnorm(500), rnorm(500) + 5])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(13,13))\n",
    "\n",
    "axes[0].scatter(x, y)\n",
    "axes[0].set_title('Data (note different axes scales)')\n",
    "\n",
    "km = KMeans(2)\n",
    "\n",
    "clusters = km.fit_predict(np.array([x, y]).T) #Clustering on non-normalized data\n",
    "\n",
    "axes[1].scatter(x, y, c=clusters, cmap='bwr')\n",
    "axes[1].set_title('Non-standardized K-means')\n",
    "\n",
    "#Scaling data using StandardScalar()\n",
    "Y = StandardScaler().fit_transform(np.array([x, y]).T)\n",
    "clusters = km.fit_predict(Y)\n",
    "axes[2].scatter(Y[:,0],Y[:,1], c=clusters, cmap='bwr')\n",
    "axes[2].set_title('Standardized K-means')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with largest scales are given more importance during dissimilarity calculation and clustering results are\n",
    "biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To scale or not to scale...**\n",
    "\n",
    "- When the units are different (eg., weight in Kg vs height in cm), always scale your data.\n",
    "- When the units are same but the features are irrelevant (eg., height of people vs height of buildings), scale your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-means is limited to linear cluster boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other clustering techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several other types of clustering algorithms better suited for different kinds of data (including the one above). Some popular clustering algorithms are:\n",
    "\n",
    "1. Spectral clustering - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html\n",
    "2. Agglomerative clustering - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "3. DBSCAN - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you cluster categorical data using K-means? How does Euclidean distance work in such a scenario?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
